{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Carlos Antonio M. Doble\n",
    "\n",
    "*Contribution statement*: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2: Data Collection with API and Web Scraping\n",
    "\n",
    "This homework will help you get used to using `requests` and `beautifulsoup4` for getting data through APIs and web scraping.\n",
    "\n",
    "Read through the whole notebook first to get an idea of what to expect and then go through each task. Skeleton code is provided for you as a guide.\n",
    "\n",
    "---\n",
    "You are allowed to work in groups (maximum of 4) to assist each other in coding problems and discussing how to handle different kinds of data. _However, you will still be submitting your notebooks **individually** and write ups (captions, answers to non-coding questions) should also be done individually._ Each notebook must have a `markdown cell` at the beginning enumerating the contributions of each member to the homework. If you worked alone, please simply state you worked alone. \n",
    "\n",
    "Missing markdown cell with contributions or any inconsistencies between statements within the group that cannot be easily reconciled by asking will **get a 5% deduction for conflicting members**.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "Read through the whole notebook first to get an idea of what is expected. \n",
    "\n",
    "After answering every question, restart your kernel and re-run ALL the cell from top to bottom to ensure that there are no errors. \n",
    "\n",
    "### Grading\n",
    "Each task will have corresponding points. Full points will be given if the task was successfully completed.\n",
    "\n",
    "The notebooks turned in should be _runnable_. I should be able to re-run your submitted notebooks from top to bottom without any errors. In case an error is encountered (that has nothing to do with FileNotFound), **there will be 5% deduction from the final score**.\n",
    "\n",
    "#### Total raw points: 30 points\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Downloading data from API\n",
    "\n",
    "Using the code from the Reddit lab exercise, choose a **different subreddit** that *you* would like to explore.\n",
    "\n",
    "Retrieve the following fields from the posts between **September 1, 2020 to September 30, 2020**.\n",
    "- author\n",
    "- subreddit\n",
    "- date created \n",
    "- number of comments \n",
    "- score\n",
    "- submission title \n",
    "- submission description\n",
    "    \n",
    "Save the data into a `pandas` `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Download the data given the time period and the selected fields. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Doing 2020-09-01 to 2020-09-02\n",
      "=====Done\n",
      "Doing 2020-09-02 to 2020-09-03\n",
      "=====Done\n",
      "Doing 2020-09-03 to 2020-09-04\n",
      "=====Done\n",
      "Doing 2020-09-04 to 2020-09-05\n",
      "=====Done\n",
      "Doing 2020-09-05 to 2020-09-06\n",
      "=====Done\n",
      "Doing 2020-09-06 to 2020-09-07\n",
      "=====Done\n",
      "Doing 2020-09-07 to 2020-09-08\n",
      "=====Done\n",
      "Doing 2020-09-08 to 2020-09-09\n",
      "=====Done\n",
      "Doing 2020-09-09 to 2020-09-10\n",
      "=====Done\n",
      "Doing 2020-09-10 to 2020-09-11\n",
      "=====Done\n",
      "Doing 2020-09-11 to 2020-09-12\n",
      "=====Done\n",
      "Doing 2020-09-12 to 2020-09-13\n",
      "=====Done\n",
      "Doing 2020-09-13 to 2020-09-14\n",
      "=====Done\n",
      "Doing 2020-09-14 to 2020-09-15\n",
      "=====Done\n",
      "Doing 2020-09-15 to 2020-09-16\n",
      "=====Done\n",
      "Doing 2020-09-16 to 2020-09-17\n",
      "=====Done\n",
      "Doing 2020-09-17 to 2020-09-18\n",
      "=====Done\n",
      "Doing 2020-09-18 to 2020-09-19\n",
      "=====Done\n",
      "Doing 2020-09-19 to 2020-09-20\n",
      "=====Done\n",
      "Doing 2020-09-20 to 2020-09-21\n",
      "=====Done\n",
      "Doing 2020-09-21 to 2020-09-22\n",
      "=====Done\n",
      "Doing 2020-09-22 to 2020-09-23\n",
      "=====Done\n",
      "Doing 2020-09-23 to 2020-09-24\n",
      "=====Done\n",
      "Doing 2020-09-24 to 2020-09-25\n",
      "=====Done\n",
      "Doing 2020-09-25 to 2020-09-26\n",
      "=====Done\n",
      "Doing 2020-09-26 to 2020-09-27\n",
      "=====Done\n",
      "Doing 2020-09-27 to 2020-09-28\n",
      "=====Done\n",
      "Doing 2020-09-28 to 2020-09-29\n",
      "=====Done\n",
      "Doing 2020-09-29 to 2020-09-30\n",
      "=====Done\n",
      "Doing 2020-09-30 to 2020-10-01\n",
      "=====Done\n"
     ]
    }
   ],
   "source": [
    "def to_utc(date):\n",
    "    #This function converts an object to UTC. This is to automate the conversion \n",
    "    #of dates instead of going to https://www.unixtimeconverter.io/ \n",
    "    return int(date.replace(tzinfo=dt.timezone.utc).timestamp())\n",
    "    \n",
    "def to_readable_date(timestamp):\n",
    "    #This function converts the UTC format to a Year-Month-Day format \n",
    "    return dt.datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#Declare start and end of reddit posts to extract \n",
    "start_date = dt.datetime.strptime(\"2020-09-01\", \"%Y-%m-%d\")\n",
    "end_date = dt.datetime.strptime(\"2020-09-30\", \"%Y-%m-%d\")\n",
    "\n",
    "#Create a range of dates to iterate \n",
    "#Note: Periods here represents the number of days it will create from the start date \n",
    "#We also do a +2 since it will only generate up to April 29. We inlcude May 1 \n",
    "#since we want to get data from the last day which is April 30 to May 1 \n",
    "date_range = (pd.date_range(\n",
    "                start_date, \n",
    "                periods=(end_date - start_date).days + 2)\n",
    "              .tolist())\n",
    "\n",
    "#prepare the parameters needed to call the API\n",
    "sort_type=\"score\"\n",
    "sort=\"desc\"\n",
    "fields=[\"author\", \"subreddit\", \"title\",\"selftext\",\"score\",\"num_comments\",\"created_utc\"]\n",
    "subreddit = 'DnD'\n",
    "url = \"https://api.pushshift.io/reddit/submission/search/\"\n",
    "results = []\n",
    "#loop through the dates \n",
    "for i, s_date in enumerate(date_range):\n",
    "    #prevents us from getting an index out of range error\n",
    "    if i != len(date_range)-1:\n",
    "        #declare end date \n",
    "        e_date = date_range[i+1]\n",
    "        #call the API\n",
    "        r = requests.get(url = url, params={\n",
    "            'after': to_utc(s_date),\n",
    "            'before': to_utc(e_date),\n",
    "            'sort_type': sort_type,\n",
    "            'sort': sort,\n",
    "            'subreddit': subreddit,\n",
    "            'fields': fields,\n",
    "            \"size\": 500\n",
    "        })\n",
    "\n",
    "        #add logs \n",
    "        print(f\"Doing {s_date.strftime('%Y-%m-%d')} to {e_date.strftime('%Y-%m-%d')}\")\n",
    "        if r.status_code == 200:\n",
    "            results.append(r.json()['data'])\n",
    "            print(\"=====Done\")\n",
    "        else:\n",
    "            print(\"=====Skipped\")\n",
    "        #so that we dont get blocked from abusing the API we call it after pausing for 1 second\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Save the results to a `pandas` `DataFrame`. (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 author  created_utc  num_comments  score  \\\n",
       "0          KymmaLabeija   1598951913           204     49   \n",
       "1          KibblesTasty   1598952378             5     31   \n",
       "2       ClockworkArcana   1598956511            21     15   \n",
       "3              Noferini   1598961239            16     15   \n",
       "4             glorycave   1598957105            21      9   \n",
       "...                 ...          ...           ...    ...   \n",
       "2995   GentleAutumnRain   1601455407             4      1   \n",
       "2996     Immortalstar01   1601427935             8      1   \n",
       "2997            Seerias   1601473949             0      1   \n",
       "2998  funk_with_dragons   1601454971             3      1   \n",
       "2999        YinWingChun   1601454037             7      1   \n",
       "\n",
       "                                               selftext subreddit  \\\n",
       "0                                                             DnD   \n",
       "1                                                             DnD   \n",
       "2     Hi all! In our campaign, we often take the roa...       DnD   \n",
       "3                                                             DnD   \n",
       "4                                                             DnD   \n",
       "...                                                 ...       ...   \n",
       "2995  I'm currently working on a new character for m...       DnD   \n",
       "2996  Reading a lot of lore, Asmodi seems to be well...       DnD   \n",
       "2997                                                          DnD   \n",
       "2998  I thought about a bioengineer or genetic scien...       DnD   \n",
       "2999  Im new to DnD and have to DM my first campaign...       DnD   \n",
       "\n",
       "                                                  title  \n",
       "0     [OC] [ART] Our party adopted a kobold and dres...  \n",
       "1                             [OC][Art] Occultist Witch  \n",
       "2     We made a free online tool for randomising tow...  \n",
       "3                  [Art] Azra Longrose - Half-Elf Rogue  \n",
       "4                          [Art] Mobius strip battlemap  \n",
       "...                                                 ...  \n",
       "2995           Need help creating a sorcerer backstory!  \n",
       "2996  How would you envision the cosmos ruled by Asm...  \n",
       "2997  HeroForge Update is a big help. (My Photoshop ...  \n",
       "2998            What if the artificer could create life  \n",
       "2999                               What are milestones?  \n",
       "\n",
       "[3000 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>created_utc</th>\n      <th>num_comments</th>\n      <th>score</th>\n      <th>selftext</th>\n      <th>subreddit</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>KymmaLabeija</td>\n      <td>1598951913</td>\n      <td>204</td>\n      <td>49</td>\n      <td></td>\n      <td>DnD</td>\n      <td>[OC] [ART] Our party adopted a kobold and dres...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>KibblesTasty</td>\n      <td>1598952378</td>\n      <td>5</td>\n      <td>31</td>\n      <td></td>\n      <td>DnD</td>\n      <td>[OC][Art] Occultist Witch</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ClockworkArcana</td>\n      <td>1598956511</td>\n      <td>21</td>\n      <td>15</td>\n      <td>Hi all! In our campaign, we often take the roa...</td>\n      <td>DnD</td>\n      <td>We made a free online tool for randomising tow...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Noferini</td>\n      <td>1598961239</td>\n      <td>16</td>\n      <td>15</td>\n      <td></td>\n      <td>DnD</td>\n      <td>[Art] Azra Longrose - Half-Elf Rogue</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>glorycave</td>\n      <td>1598957105</td>\n      <td>21</td>\n      <td>9</td>\n      <td></td>\n      <td>DnD</td>\n      <td>[Art] Mobius strip battlemap</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2995</th>\n      <td>GentleAutumnRain</td>\n      <td>1601455407</td>\n      <td>4</td>\n      <td>1</td>\n      <td>I'm currently working on a new character for m...</td>\n      <td>DnD</td>\n      <td>Need help creating a sorcerer backstory!</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>Immortalstar01</td>\n      <td>1601427935</td>\n      <td>8</td>\n      <td>1</td>\n      <td>Reading a lot of lore, Asmodi seems to be well...</td>\n      <td>DnD</td>\n      <td>How would you envision the cosmos ruled by Asm...</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>Seerias</td>\n      <td>1601473949</td>\n      <td>0</td>\n      <td>1</td>\n      <td></td>\n      <td>DnD</td>\n      <td>HeroForge Update is a big help. (My Photoshop ...</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>funk_with_dragons</td>\n      <td>1601454971</td>\n      <td>3</td>\n      <td>1</td>\n      <td>I thought about a bioengineer or genetic scien...</td>\n      <td>DnD</td>\n      <td>What if the artificer could create life</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>YinWingChun</td>\n      <td>1601454037</td>\n      <td>7</td>\n      <td>1</td>\n      <td>Im new to DnD and have to DM my first campaign...</td>\n      <td>DnD</td>\n      <td>What are milestones?</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# your code for saving data into a DataFrame here\n",
    "# store it also in a CSV file for use later (maybe in the next assignment!)\n",
    "flat_list = []\n",
    "#loop through the reddit results\n",
    "for sublist in results:\n",
    "    #check if sublist is not empty. The reason we have empty lists is because there are days wherein there are no submissions\n",
    "    if sublist is not None:\n",
    "        #for each dictionary in the sublist add it to the flat list \n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "\n",
    "#pandas has a useful function called from_dict which will convert a list of dictionary objects into a dataframe\n",
    "df = pd.DataFrame.from_dict(flat_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. How many posts were you able to retrieve? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# use a pandas function or a python function to get the size of your dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. Web Scraping Books\n",
    "\n",
    "Go to http://books.toscrape.com/, using what you have learned create a CSV file the contains all the books found in the website. The CSV file should contain the following:\n",
    "- Title\n",
    "- Price\n",
    "- Description\n",
    "- Availability\n",
    "\n",
    "Code guides have been provided to help you in creating the web scraper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://books.toscrape.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Complete the `get_title_links_and_next_page` function. (3 pts)\n",
    "This function returns 2 things: the **book urls in a page** and the **link to the next page**. \n",
    "\n",
    "The idea here is to collect first all the book links available in the website and store the links in the `title_links` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_links_and_next_page(page_url):\n",
    "    #this is where we store our links to the title \n",
    "    list_links = [] \n",
    "    #get the html for the url that was given\n",
    "    page = requests.get(page_url)\n",
    "    \n",
    "    #parse the html file for beautifulsoup to query on\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    #inspecting the page we notice that the books are placed under \n",
    "    #the article tag so we get all articles\n",
    "    for article in soup.find_all('article'):\n",
    "        #the article tag has an anchor tag so we find it and get the href\n",
    "        if \"catalogue\" not in article.find(\"a\")['href']:\n",
    "            url = base_url + \"catalogue/\" + article.find(\"a\")['href']\n",
    "        else:\n",
    "            url = base_url + article.find(\"a\")['href']\n",
    "        #add the title url to our list of titles \n",
    "        list_links.append(url)\n",
    "    \n",
    "    #try to check if a next button is in the page \n",
    "    try:\n",
    "        next_url = \n",
    "    #if none we return None :)     \n",
    "    except:\n",
    "        next_url = None\n",
    "\n",
    "    return (list_links, next_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Complete the link collector. (2 pts)\n",
    "\n",
    "This code block is your starter scraper. It uses the data returned by the function `get_title_links_and_next_page` to go through all the pages in http://books.toscrape.com/index.html.\n",
    "\n",
    "Complete the lines marked with `# TODO!!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial set up to crawl the book links and next page\n",
    "res = get_title_links_and_next_page('http://books.toscrape.com/index.html')\n",
    "title_links = res[0]  \n",
    "\n",
    "#while we get a next page link keep on crawling for book links\n",
    "while res[1]:\n",
    "    #there are cases that the word \"catalogue\" is not in the link so we add it \n",
    "    #so that we can crawl properly\n",
    "    if \"catalogue\" not in res[1]:\n",
    "        page_url = base_url + \"catalogue/\" + res[1]\n",
    "    else:\n",
    "        page_url = base_url + res[1]\n",
    "    res = # TODO!!\n",
    "    title_links += # TODO!!\n",
    "\n",
    "title_links  # this should print a list of every book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Complete the functions. (8 pts)\n",
    "Once you have a list of all the available book links, we can now loop through the links and use the 4 functions `get_title`, `get_price`, `get_description`, `get_availability` to retrieve the book information.\n",
    "\n",
    "Complete the functions below to get the specific fields from the individual links from `title_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    return \n",
    "\n",
    "def get_price(soup):\n",
    "    return \n",
    "    \n",
    "def get_description(soup):\n",
    "    return \n",
    "    \n",
    "def get_availability(soup):\n",
    "    return \n",
    "\n",
    "\n",
    "# This is the scraper for each and every HTML page in title_links\n",
    "book_data = []\n",
    "for title_link in : \n",
    "    page = \n",
    "    soup = \n",
    "    \n",
    "    title = get_title(soup)\n",
    "    price = get_price(soup)\n",
    "    description = get_description(soup)\n",
    "    availability = get_availability(soup)\n",
    "    \n",
    "    book_data += [[title, price, description, availability.strip()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7. Save the data into a `pandas` `DataFrame`. (2 pts)\n",
    "\n",
    "Pass the correct data value to convert the collected books into a `DataFrame` and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=)\n",
    "df.columns = ['title', 'price', 'description', 'availability']\n",
    "display(df.head())\n",
    "\n",
    "#save to csv file \n",
    "df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>= END =</center></h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}