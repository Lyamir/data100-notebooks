{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: This notebook was made for the Principles of Data Science course (DATA100).*\n",
    "\n",
    "_This is not for distribution oustide of this class._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection through APIs\n",
    "\n",
    "Sadly, there are times wherein data will not be handed on a silver platter there are times wherein you have to gather data yourselves. Gaining exposure through various data collection methods will be a vital skill to have in order to thrive as a budding data scientist. \n",
    "\n",
    "In this notebook, we will be gathering data from through an Application Programming Interface(API). An API is an interface that allows programs to communicate with each other. A wide use of APIs can be found on online banking transactions. Usually, banks would expose APIs that allow developers cerain access to the bank's system (ex. money transfer, balance checking). In turn, developers will use these APIs to build the functionalities of their applications. In our case we use APIs to extract data to do our analysis and build our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2400/1*vhoE-Yw2HgrlScZmR_L1zA.gif\"/>Sourced from <a href=\"https://medium.com/@JanisGraubins/open-banking-api-explained-in-3-gifs-b806f14ca2ca\"> Grabuis Jannis</a></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import csv\n",
    "import os \n",
    "import requests \n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To familiarize ourselves with the use APIs we will be extracting data from <a href=\"reddit.com\">reddit</a>. \n",
    "\n",
    "**Reddit** is a media aggregation website that contains multiple communities. You guys can imagine it as a \"forum-esque\" website where users can discuss about anything under the sun. The tool that we will be using to get reddit data is <a href=\"https://pushshift.io/\"> pushift</a>. \n",
    "\n",
    "**Pushshift**, is an open source big data storage platform that copies submissions on reddit. The developer of this project has exposed an API which allows users to search for comments, aggregate data, query data for a specific range and many <a href=\"https://www.reddit.com/r/pushshift/comments/bcxguf/new_to_pushshift_read_this_faq/\">more</a>. One *limitation* of the tool is that it saves the metadata of the submissions the time it was posted on reddit therefore **scores, upvotes and downvotes are not updated**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access pushshift we will be using the `requests` library of Python. This library allows us to do HTTP requests on the API. Basically, this library allows us to communicate with the API. \n",
    "\n",
    "There are 3 main entry to points for the API these are: <br>\n",
    "`/reddit/comment/search` <br>\n",
    "`/reddit/submission/search` <br>\n",
    "`/reddit/subreddit/search` <br>\n",
    "\n",
    "This means you can search based on texts coming from comments, submissions or subreddit. To filter your query the API offers a `param ` attribute. There are various parameters which you can find <a href=\"https://github.com/pushshift/api\">here</a>. For this tutorial we will only be using 7. <br>\n",
    "\n",
    "`after` - filter results to those made after a given date <br>\n",
    "`before`- filter results to those made before a given date <br>\n",
    "`sort_type` - sort by a specific attribute default value is \"created_utc\" other options are  (\"score\", \"num_comments\", \"created_utc\") <br>\n",
    "`sort` - sort results in specific order (\"asc\" or \"desc\") <br>\n",
    "`subreddit` - name of subreddit <br>\n",
    "`size` - number of results to return <br>\n",
    "`fields` - specifcy which fields to return <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample API Query \n",
    "The query below gets submissions from DLSU's <a href=\"https://www.reddit.com/r/dlsu/\">reddit</a> community from November 1, 2020 to November 7, 2020 sorted in descending order based on score with only 10 results. We also indicate what fields the API should return. \n",
    "\n",
    "If you notice there is something weird about the date we placed in the `after` and `before` parameters. The API uses an epoch timestamp. This is the timestamp used by Unix. For us to interface with the API properly we can use this <a href=\"https://www.epochconverter.com/\">time converter</a>. Just type in the date that you want to be converted to Unix. \n",
    "\n",
    "You may notice that after calling the requests library we use the `.json` method. This converts the results of the request into a JSON object. JSON is a widely used file format usually used to communicate between applications. In our case Reddit API and this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://api.pushshift.io/reddit/submission/search/\"  #query submissions\n",
    "PARAMS = {\n",
    "    \n",
    "    'after': 1604102400, #get dates after October 31, 2020\n",
    "    'before': 1604793600, #get dates before November 8, 2020\n",
    "    'sort_type': 'score', # sort by score\n",
    "    'sort': 'desc', # sort in descending order\n",
    "    'subreddit': 'dlsu', # do a search on DLSU subreddit\n",
    "    'size': 10, # give only 10 search results\n",
    "#     'fields': [\"id\",\"title\",\"selftext\",\"score\",\"num_comments\",\"created_utc\"] #return only the following fields\n",
    "}\n",
    "\n",
    "#use the requests library to query pushshift api\n",
    "r = requests.get(url = URL, params=PARAMS)\n",
    "#parse returned data to a json object\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = r.json()\n",
    "len(posts['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the API Query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to scale the API and be able to get all reddit posts from October 1, 2020 to October 31, 2020? One way to do this would be to call the API for each day from October 1 to October 31. For each day we set the size of the results to 500. The reason we are limiting it to 500 is the API only accepts values <= 500 for the size parameter. \n",
    "\n",
    "Note: <br>\n",
    "We can see a limitation in this approach, what if a reddit page gets more than 500 posts a day. This a reality that you may encounter when dealing with APIs there will be limitations in how much you can query or extract. In the case of reddit, pushshift has provided a data dump through this <a href=\"https://files.pushshift.io/reddit/\">link</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_utc(date):\n",
    "    #This function converts an object to UTC. This is to automate the conversion \n",
    "    #of dates instead of going to https://www.unixtimeconverter.io/ \n",
    "    return int(date.replace(tzinfo=dt.timezone.utc).timestamp())\n",
    "    \n",
    "def to_readable_date(timestamp):\n",
    "    #This function converts the UTC format to a Year-Month-Day format \n",
    "    return dt.datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#Declare start and end of reddit posts to extract \n",
    "start_date = dt.datetime.strptime(\"2020-10-01\", \"%Y-%m-%d\")\n",
    "end_date = dt.datetime.strptime(\"2020-10-31\", \"%Y-%m-%d\")\n",
    "\n",
    "#Create a range of dates to iterate \n",
    "#Note: Periods here represents the number of days it will create from the start date \n",
    "#We also do a +2 since it will only generate up to April 29. We inlcude May 1 \n",
    "#since we want to get data from the last day which is April 30 to May 1 \n",
    "date_range = (pd.date_range(\n",
    "                start_date, \n",
    "                periods=(end_date - start_date).days + 2)\n",
    "              .tolist())\n",
    "\n",
    "#prepare the parameters needed to call the API\n",
    "sort_type=\"score\"\n",
    "sort=\"desc\"\n",
    "fields=[\"id\",\"title\",\"selftext\",\"score\",\"num_comments\",\"created_utc\"]\n",
    "subreddit = 'dlsu'\n",
    "url = \"https://api.pushshift.io/reddit/submission/search/\"\n",
    "results = []\n",
    "#loop through the dates \n",
    "for i, s_date in enumerate(date_range):\n",
    "    #prevents us from getting an index out of range error\n",
    "    if i != len(date_range)-1:\n",
    "        #declare end date \n",
    "        e_date = date_range[i+1]\n",
    "        #call the API\n",
    "        r = requests.get(url = url, params={\n",
    "            'after': to_utc(s_date),\n",
    "            'before': to_utc(e_date),\n",
    "            'sort_type': sort_type,\n",
    "            'sort': sort,\n",
    "            'subreddit': subreddit,\n",
    "            'fields': fields,\n",
    "            \"size\": 500\n",
    "        })\n",
    "\n",
    "        #add logs \n",
    "        print(f\"Doing {s_date.strftime('%Y-%m-%d')} to {e_date.strftime('%Y-%m-%d')}\")\n",
    "        if r.status_code == 200:\n",
    "            results.append(r.json()['data'])\n",
    "            print(\"=====Done\")\n",
    "        else:\n",
    "            print(\"=====Skipped\")\n",
    "        #so that we dont get blocked from abusing the API we call it after pausing for 1 second\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results variable will be a list of lists and within a list is a dictionary object. Now for us to properly save this to a dataframe we would need to flatten the list and create a list of dictionary objects instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data into a table\n",
    "\n",
    "Given that the JSON representation is a bit complex, additional processing would need to be done to be able to reogranize or restructure the data into a \"flat\" list of dictionary objects.\n",
    "\n",
    "Once we have the flat list, we can easily pass this to `pandas` which will make our data easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = []\n",
    "#loop through the reddit results\n",
    "for sublist in results:\n",
    "    #check if sublist is not empty. The reason we have empty lists is because there are days wherein there are no submissions\n",
    "    if sublist is not None:\n",
    "        #for each dictionary in the sublist add it to the flat list \n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "\n",
    "#pandas has a useful function called from_dict which will convert a list of dictionary objects into a dataframe\n",
    "df = pd.DataFrame.from_dict(flat_list)\n",
    "display(df.head())\n",
    "df.to_csv(\"reddit_dlsu.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
