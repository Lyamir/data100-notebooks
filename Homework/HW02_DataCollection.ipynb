{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: \n",
    "\n",
    "*Contribution statement*: I worked on this alone or I worked on this with A, B and C. I did task x while B worked on task y..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2: Data Collection with API and Web Scraping\n",
    "\n",
    "This homework will help you get used to using `requests` and `beautifulsoup4` for getting data through APIs and web scraping.\n",
    "\n",
    "Read through the whole notebook first to get an idea of what to expect and then go through each task. Skeleton code is provided for you as a guide.\n",
    "\n",
    "---\n",
    "You are allowed to work in groups (maximum of 4) to assist each other in coding problems and discussing how to handle different kinds of data. _However, you will still be submitting your notebooks **individually** and write ups (captions, answers to non-coding questions) should also be done individually._ Each notebook must have a `markdown cell` at the beginning enumerating the contributions of each member to the homework. If you worked alone, please simply state you worked alone. \n",
    "\n",
    "Missing markdown cell with contributions or any inconsistencies between statements within the group that cannot be easily reconciled by asking will **get a 5% deduction for conflicting members**.\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "Read through the whole notebook first to get an idea of what is expected. \n",
    "\n",
    "After answering every question, restart your kernel and re-run ALL the cell from top to bottom to ensure that there are no errors. \n",
    "\n",
    "### Grading\n",
    "Each task will have corresponding points. Full points will be given if the task was successfully completed.\n",
    "\n",
    "The notebooks turned in should be _runnable_. I should be able to re-run your submitted notebooks from top to bottom without any errors. In case an error is encountered (that has nothing to do with FileNotFound), **there will be 5% deduction from the final score**.\n",
    "\n",
    "#### Total raw points: 30 points\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Downloading data from API\n",
    "\n",
    "Using the code from the Reddit lab exercise, choose a **different subreddit** that *you* would like to explore.\n",
    "\n",
    "Retrieve the following fields from the posts between **September 1, 2020 to September 30, 2020**.\n",
    "- author\n",
    "- subreddit\n",
    "- date created \n",
    "- number of comments \n",
    "- score\n",
    "- submission title \n",
    "- submission description\n",
    "    \n",
    "Save the data into a `pandas` `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Download the data given the time period and the selected fields. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the code from the lab exercise (more or less)\n",
    "# make the necessary changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Save the results to a `pandas` `DataFrame`. (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for saving data into a DataFrame here\n",
    "# store it also in a CSV file for use later (maybe in the next assignment!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. How many posts were you able to retrieve? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a pandas function or a python function to get the size of your dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. Web Scraping Books\n",
    "\n",
    "Go to http://books.toscrape.com/, using what you have learned create a CSV file the contains all the books found in the website. The CSV file should contain the following:\n",
    "- Title\n",
    "- Price\n",
    "- Description\n",
    "- Availability\n",
    "\n",
    "Code guides have been provided to help you in creating the web scraper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://books.toscrape.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Complete the `get_title_links_and_next_page` function. (3 pts)\n",
    "This function returns 2 things: the **book urls in a page** and the **link to the next page**. \n",
    "\n",
    "The idea here is to collect first all the book links available in the website and store the links in the `title_links` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_links_and_next_page(page_url):\n",
    "    #this is where we store our links to the title \n",
    "    list_links = [] \n",
    "    #get the html for the url that was given\n",
    "    page = # TODO!!\n",
    "    \n",
    "    #parse the html file for beautifulsoup to query on\n",
    "    soup = # TODO!!\n",
    "    \n",
    "    #inspecting the page we notice that the books are placed under \n",
    "    #the article tag so we get all articles\n",
    "    for article in soup.find_all('article'):\n",
    "        #the article tag has an anchor tag so we find it and get the href\n",
    "        if \"catalogue\" not in article.find(\"a\")['href']:\n",
    "            url = base_url + \"catalogue/\" + article.find(\"a\")['href']\n",
    "        else:\n",
    "            url = base_url + article.find(\"a\")['href']\n",
    "        #add the title url to our list of titles \n",
    "        list_links.append(url)\n",
    "    \n",
    "    #try to check if a next button is in the page \n",
    "    try:\n",
    "        next_url = # TODO!!\n",
    "    #if none we return None :)     \n",
    "    except:\n",
    "        next_url = None\n",
    "\n",
    "    return (list_links, next_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Complete the link collector. (2 pts)\n",
    "\n",
    "This code block is your starter scraper. It uses the data returned by the function `get_title_links_and_next_page` to go through all the pages in http://books.toscrape.com/index.html.\n",
    "\n",
    "Complete the lines marked with `# TODO!!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial set up to crawl the book links and next page\n",
    "res = get_title_links_and_next_page('http://books.toscrape.com/index.html')\n",
    "title_links = res[0]  \n",
    "\n",
    "#while we get a next page link keep on crawling for book links\n",
    "while res[1]:\n",
    "    #there are cases that the word \"catalogue\" is not in the link so we add it \n",
    "    #so that we can crawl properly\n",
    "    if \"catalogue\" not in res[1]:\n",
    "        page_url = base_url + \"catalogue/\" + res[1]\n",
    "    else:\n",
    "        page_url = base_url + res[1]\n",
    "    res = # TODO!!\n",
    "    title_links += # TODO!!\n",
    "\n",
    "title_links  # this should print a list of every book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Complete the functions. (8 pts)\n",
    "Once you have a list of all the available book links, we can now loop through the links and use the 4 functions `get_title`, `get_price`, `get_description`, `get_availability` to retrieve the book information.\n",
    "\n",
    "Complete the functions below to get the specific fields from the individual links from `title_links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    return \n",
    "\n",
    "def get_price(soup):\n",
    "    return \n",
    "    \n",
    "def get_description(soup):\n",
    "    return \n",
    "    \n",
    "def get_availability(soup):\n",
    "    return \n",
    "\n",
    "\n",
    "# This is the scraper for each and every HTML page in title_links\n",
    "book_data = []\n",
    "for title_link in : \n",
    "    page = \n",
    "    soup = \n",
    "    \n",
    "    title = get_title(soup)\n",
    "    price = get_price(soup)\n",
    "    description = get_description(soup)\n",
    "    availability = get_availability(soup)\n",
    "    \n",
    "    book_data += [[title, price, description, availability.strip()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7. Save the data into a `pandas` `DataFrame`. (2 pts)\n",
    "\n",
    "Pass the correct data value to convert the collected books into a `DataFrame` and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=)\n",
    "df.columns = ['title', 'price', 'description', 'availability']\n",
    "display(df.head())\n",
    "\n",
    "#save to csv file \n",
    "df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>= END =</center></h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
